{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace5cd0e",
   "metadata": {},
   "source": [
    "–î–ª—è  –Ω–∞—à–µ–π –∑–∞–¥–∞—á–∏, –∫—Ä–∞–π–Ω–µ –≤–∞–∂–Ω–æ  –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö, –∫ —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ –∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä —è –ø–æ–ª—É—á–∏–ª –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ—à–∏–±–∫–∏ –∏ –Ω–µ —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "1) –û—á–µ–Ω—å –∫—Ä–∏–≤–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏\n",
    "2) –û—á–µ–Ω—å —Ç—Ä—É–¥–Ω–æ —Ä–∞–∑–±–∏–≤–∞—Ç—å –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å  –Ω–æ–¥—ã –ø–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –º—ã –∏—Ö –∑–∞–ø—É—Å—Ç–∏–ª–∏ –≤ —É–±–æ–≥–æ–º –≤–∏–¥–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö\n",
    "3) –ù—É–∂–Ω–æ –∏–º–µ—Ç—å —á–µ—Ç–∫–∏–π –∏ —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–π–º—ã–π  –∫–æ–¥ , –∞ —Ç–∞–∫–∂–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–æ–¥–æ–≤ –≤ –±–¥ \n",
    "4) –ù–µ –∏–º–µ–µ–º —Å—Ç–∞–Ω–∞–¥—Ä—Ç–∞ pdf —Ñ–∞–π–ª–æ–≤, –ø—Ä–æ–±–ª–µ–º—É –ø–æ—á—Ç–∏ –Ω–µ —Ä–µ–∞–ª—å–Ω–æ —Ä–µ—à–∏—Ç—å,  –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –Ω–∞–ø–∏—Å–∞—Ç—å —á—Ç–æ-—Ç–æ –ø–æ—Ö–æ–∂ –Ω–∞ —Å–∫—Ä–∏–ø—Ç –æ—Ç—á–∏—Å—Ç–∫–∏ –ø—Ä–µ–¥—É—Å–º–∞—Ç—Ä–∏–≤–∞—é—â–∏–π –º–Ω–æ–∂–µ—Å—Ç–≤–æ  –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤\n",
    "\n",
    "\n",
    "–¢–æ–≥–¥–∞, –∫–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —Ç—É—Ç –ø–æ–ø—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å –∫—Ä–∞–Ω–π–µ —á–µ—Ç–∫–æ–µ –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, —Å –ø–æ–Ω—è—Ç–Ω—ã–º —Ä–∞–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ —Ç–∏–∫–∏—Ç—ã, —Ç–∞–∫ –∂–µ –ø—Ä–µ–¥–ª–∞–≥–∞—é —É—Å–ª–æ–∂–Ω–∏—Ç—å —Å–∞–º–æ–µ —Ä–∞–±–æ—á–µ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—É—Ü—Ç–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–∞–∏–≤–∏–º–æ—Å—Ç–µ–π\n",
    "\n",
    "–ù–∞—à —Å—Ç–µ–∫:\n",
    "1) LangChain\n",
    "2) GigaChat\n",
    "3) Neo4j\n",
    "4) Numpay/Pandas\n",
    "5) NTLK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477e919",
   "metadata": {},
   "source": [
    "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ pdf —Ñ–∞–π–ª–∞ —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∏ –ø–æ–Ω—Ç—è–Ω–æ–π —Ä–∞–∑–±–∏–≤–∫–æ–π —á–∞–Ω–∫–æ–≤\n",
    "–ó–∞–¥–∞—á–∏:\n",
    "1) –ü—Ä–æ—Ç–∏—Å—å –ø–æ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ –ø—Ä–æ–≤–µ—Ä–∏–≤ –µ–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å\n",
    "2) –û–ë—Ä–µ–∑–∞—Ç—å  —Ö–æ—Ç—è –±—ã –¥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∞ –Ω–µ –∫–∞–∫ —Ä–∞–Ω—å—à–µ\n",
    "3) –ú–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ä–∞–∑—É –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–æ–±—Ä–∞—Ç—å –∫–∞–∫—É—é-—Ç–æ –∫–ª—é—á–µ–≤—É–± –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4050ee0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b211dc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ –í–∞—Ä–∏–∞–Ω—Ç 1: –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\n",
      "üìñ –ó–∞–≥—Ä—É–∂–∞—é PDF: ../book2.pdf\n",
      "üìÑ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ 10 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
      "üßπ –û—á–∏—â–∞—é —Ç–µ–∫—Å—Ç...\n",
      "‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞–Ω–∫–∏ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: pages)...\n",
      "‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 10 —Å—Ç—Ä–∞–Ω–∏—Ü -> 10 —á–∞–Ω–∫–æ–≤\n",
      "   –ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ (219 —Å–∏–º–≤–æ–ª–æ–≤):\n",
      "   '–ê–ª–µ–∫—Å–∞–Ω–æ—Ä –ö–†–ò–í–ò–õ–ï–í –ö–û–ú–ü–¨–Æ–¢–ï–†–ù–û–ô –ú–ê–¢–ï–ú–ê–¢–ò–ö–ò –∑ —Ñ—É–ø—Å–π–æ–ª [ —Å–∏—Å–Å–µ[–≥,—Ö–û,—É0) –ö–æ. –°–í–°–ï —Å–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–Ω–æ—Å—Ç–∏ –≥...'\n",
      "\n",
      "============================================================\n",
      "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:\n",
      "   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/—á–∞–Ω–∫–æ–≤: 10\n",
      "   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: 15,411\n",
      "   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: 1541 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 219\n",
      "   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 2319\n",
      "============================================================\n",
      "\n",
      "üéØ –í–∞—Ä–∏–∞–Ω—Ç 2: –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º\n",
      "üìñ –ó–∞–≥—Ä—É–∂–∞—é PDF: ../book2.pdf\n",
      "üìÑ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ 10 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
      "üßπ –û—á–∏—â–∞—é —Ç–µ–∫—Å—Ç...\n",
      "‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞–Ω–∫–∏ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: sentences)...\n",
      "‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 10 —Å—Ç—Ä–∞–Ω–∏—Ü -> 29 —á–∞–Ω–∫–æ–≤\n",
      "   –ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ (219 —Å–∏–º–≤–æ–ª–æ–≤):\n",
      "   '–ê–ª–µ–∫—Å–∞–Ω–æ—Ä –ö–†–ò–í–ò–õ–ï–í –ö–û–ú–ü–¨–Æ–¢–ï–†–ù–û–ô –ú–ê–¢–ï–ú–ê–¢–ò–ö–ò –∑ —Ñ—É–ø—Å–π–æ–ª [ —Å–∏—Å–Å–µ[–≥,—Ö–û,—É0) –ö–æ. –°–í–°–ï —Å–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–Ω–æ—Å—Ç–∏ –≥...'\n",
      "\n",
      "============================================================\n",
      "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:\n",
      "   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/—á–∞–Ω–∫–æ–≤: 29\n",
      "   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: 17,888\n",
      "   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: 617 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 102\n",
      "   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 1515\n",
      "============================================================\n",
      "\n",
      "üéØ –í–∞—Ä–∏–∞–Ω—Ç 3: –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ\n",
      "üìñ –ó–∞–≥—Ä—É–∂–∞—é PDF: ../book2.pdf\n",
      "üìÑ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ 10 —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\n",
      "üßπ –û—á–∏—â–∞—é —Ç–µ–∫—Å—Ç...\n",
      "‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞–Ω–∫–∏ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: smart)...\n",
      "üìÑ –ò—Å–ø–æ–ª—å–∑—É—é —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\n",
      "‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 10 —Å—Ç—Ä–∞–Ω–∏—Ü -> 10 —á–∞–Ω–∫–æ–≤\n",
      "   –ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ (219 —Å–∏–º–≤–æ–ª–æ–≤):\n",
      "   '–ê–ª–µ–∫—Å–∞–Ω–æ—Ä –ö–†–ò–í–ò–õ–ï–í –ö–û–ú–ü–¨–Æ–¢–ï–†–ù–û–ô –ú–ê–¢–ï–ú–ê–¢–ò–ö–ò –∑ —Ñ—É–ø—Å–π–æ–ª [ —Å–∏—Å–Å–µ[–≥,—Ö–û,—É0) –ö–æ. –°–í–°–ï —Å–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–Ω–æ—Å—Ç–∏ –≥...'\n",
      "\n",
      "============================================================\n",
      "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:\n",
      "   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/—á–∞–Ω–∫–æ–≤: 10\n",
      "   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: 15,411\n",
      "   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: 1541 —Å–∏–º–≤–æ–ª–æ–≤\n",
      "   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 219\n",
      "   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: 2319\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    –û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –º—É—Å–æ—Ä–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã.\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.,!?;:()\\-‚Äî\\[\\]{}\"\\']', '', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+([.,!?;:])', r'\\1', text)\n",
    "    text = re.sub(r'([.,!?;:])\\s+', r'\\1 ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def is_valid_line(line):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–∞–ª–∏–¥–Ω–æ–π (–Ω–µ –º—É—Å–æ—Ä–Ω–æ–π).\n",
    "    \"\"\"\n",
    "\n",
    "    if len(line.strip()) < 3:\n",
    "        return False\n",
    "    if not re.search(r'[–∞-—è–ê-–Øa-zA-Z0-9]', line):\n",
    "        return False\n",
    "    if re.fullmatch(r'[^–∞-—è–ê-–Øa-zA-Z0-9]+', line):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def clean_document_pages(documents):\n",
    "    \"\"\"\n",
    "    –û—á–∏—â–∞–µ—Ç –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–æ–∫—É–º–µ–Ω—Ç–∞, —É–±–∏—Ä–∞—è –º—É—Å–æ—Ä –∏ –ø—Ä–æ–≤–µ—Ä—è—è —Å—Ç—Ä–æ–∫–∏.\n",
    "    \"\"\"\n",
    "    cleaned_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        if hasattr(doc, 'page_content'):\n",
    "            text = doc.page_content\n",
    "            metadata = doc.metadata.copy() if hasattr(doc, 'metadata') else {}\n",
    "        else:\n",
    "            text = doc\n",
    "            metadata = {}\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if is_valid_line(line):\n",
    "                cleaned_line = clean_text(line)\n",
    "                if cleaned_line:\n",
    "                    cleaned_lines.append(cleaned_line)\n",
    "        \n",
    "        \n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ\n",
    "        if cleaned_lines:\n",
    "            cleaned_text = ' '.join(cleaned_lines)\n",
    "            cleaned_docs.append(Document(\n",
    "                page_content=cleaned_text,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "    \n",
    "    return cleaned_docs\n",
    "\n",
    "def split_by_sentences(documents, max_sentences_per_chunk=10, sentence_overlap=2):\n",
    "    \"\"\"\n",
    "    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, –Ω–µ –æ–±—Ä–µ–∑–∞—è –∏—Ö.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        text = doc.page_content\n",
    "        metadata = doc.metadata.copy()\n",
    "        \n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "        sentences = sent_tokenize(text, language='russian')\n",
    "        \n",
    "        # –ï—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –º–∞–ª–æ, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å\n",
    "        if len(sentences) <= max_sentences_per_chunk:\n",
    "            all_chunks.append(doc)\n",
    "            continue\n",
    "        \n",
    "        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º\n",
    "        for i in range(0, len(sentences), max_sentences_per_chunk - sentence_overlap):\n",
    "            chunk_sentences = sentences[i:i + max_sentences_per_chunk]\n",
    "            if chunk_sentences:\n",
    "                chunk_text = ' '.join(chunk_sentences)\n",
    "                chunk_metadata = metadata.copy()\n",
    "                chunk_metadata['sentence_start'] = i\n",
    "                chunk_metadata['sentence_end'] = i + len(chunk_sentences) - 1\n",
    "                chunk_metadata['total_sentences'] = len(sentences)\n",
    "                \n",
    "                all_chunks.append(Document(\n",
    "                    page_content=chunk_text,\n",
    "                    metadata=chunk_metadata\n",
    "                ))\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def split_by_pages(documents):\n",
    "    \"\"\"\n",
    "    –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –æ—Ç–¥–µ–ª—å–Ω—ã–π —á–∞–Ω–∫).\n",
    "    \"\"\"\n",
    "    # –í –≤–∞—à–µ–º —Å–ª—É—á–∞–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\n",
    "    # –ó–¥–µ—Å—å –º—ã –ø—Ä–æ—Å—Ç–æ —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —É –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –µ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "    page_chunks = []\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        metadata = doc.metadata.copy() if hasattr(doc, 'metadata') else {}\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º/–æ–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ\n",
    "        if 'page' not in metadata:\n",
    "            metadata['page'] = i + 1\n",
    "        \n",
    "        page_chunks.append(Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata=metadata\n",
    "        ))\n",
    "    \n",
    "    return page_chunks\n",
    "\n",
    "def smart_split_documents(documents, chunk_size=1000, chunk_overlap=200, strategy='smart'):\n",
    "    \"\"\"\n",
    "    –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤—ã–±–æ—Ä–æ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.\n",
    "    \n",
    "    –°—Ç—Ä–∞—Ç–µ–≥–∏–∏:\n",
    "    - 'smart': –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π —Å–ø–æ—Å–æ–±\n",
    "    - 'pages': —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\n",
    "    - 'sentences': —Ä–∞–∑–±–∏–≤–∞–µ—Ç –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º\n",
    "    - 'recursive': –∏—Å–ø–æ–ª—å–∑—É–µ—Ç RecursiveCharacterTextSplitter\n",
    "    \"\"\"\n",
    "    \n",
    "    # –°–Ω–∞—á–∞–ª–∞ –æ—á–∏—â–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã\n",
    "    cleaned_docs = clean_document_pages(documents)\n",
    "    \n",
    "    if strategy == 'pages':\n",
    "        return split_by_pages(cleaned_docs)\n",
    "    \n",
    "    elif strategy == 'sentences':\n",
    "        return split_by_sentences(cleaned_docs)\n",
    "    \n",
    "    elif strategy == 'smart':\n",
    "\n",
    "        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞\n",
    "        all_text = ' '.join([doc.page_content for doc in cleaned_docs])\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\n",
    "        if len(cleaned_docs) > 1 and all('page' in doc.metadata for doc in cleaned_docs):\n",
    "            print(\"üìÑ –ò—Å–ø–æ–ª—å–∑—É—é —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\")\n",
    "            return split_by_pages(cleaned_docs)\n",
    "        \n",
    "        # –ò–Ω–∞—á–µ –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º\n",
    "        print(\"üî§ –ò—Å–ø–æ–ª—å–∑—É—é —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º\")\n",
    "        return split_by_sentences(cleaned_docs)\n",
    "    \n",
    "    else:  # 'recursive' –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "            length_function=len,\n",
    "        )\n",
    "        return text_splitter.split_documents(cleaned_docs)\n",
    "\n",
    "def load_and_prepare_pdfs(file_paths, max_pages=None, chunk_strategy='smart'):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç PDF —Ñ–∞–π–ª—ã, –æ—á–∏—â–∞–µ—Ç –∏ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.pdf'):\n",
    "            print(f\"üìñ –ó–∞–≥—Ä—É–∂–∞—é PDF: {file_path}\")\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            loaded_docs = loader.load()\n",
    "            \n",
    "            if max_pages is not None and max_pages > 0:\n",
    "                loaded_docs = loaded_docs[:max_pages]\n",
    "                print(f\"üìÑ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–æ {max_pages} —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏\")\n",
    "            \n",
    "            print(f\"üßπ –û—á–∏—â–∞—é —Ç–µ–∫—Å—Ç...\")\n",
    "            cleaned_docs = clean_document_pages(loaded_docs)\n",
    "            \n",
    "            print(f\"‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞–Ω–∫–∏ (—Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {chunk_strategy})...\")\n",
    "            chunks = smart_split_documents(cleaned_docs, strategy=chunk_strategy)\n",
    "            \n",
    "            all_documents.extend(chunks)\n",
    "            \n",
    "            print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(loaded_docs)} —Å—Ç—Ä–∞–Ω–∏—Ü -> {len(chunks)} —á–∞–Ω–∫–æ–≤\")\n",
    "            print(f\"   –ü—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞ ({len(chunks[0].page_content) if chunks else 0} —Å–∏–º–≤–æ–ª–æ–≤):\")\n",
    "            if chunks and len(chunks[0].page_content) > 100:\n",
    "                print(f\"   '{chunks[0].page_content[:100]}...'\")\n",
    "            print()\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    total_chars = sum(len(doc.page_content) for doc in all_documents)\n",
    "    avg_chunk_size = total_chars / len(all_documents) if all_documents else 0\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:\")\n",
    "    print(f\"   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/—á–∞–Ω–∫–æ–≤: {len(all_documents)}\")\n",
    "    print(f\"   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {total_chars:,}\")\n",
    "    print(f\"   –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {avg_chunk_size:.0f} —Å–∏–º–≤–æ–ª–æ–≤\")\n",
    "    print(f\"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {min(len(doc.page_content) for doc in all_documents) if all_documents else 0}\")\n",
    "    print(f\"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max(len(doc.page_content) for doc in all_documents) if all_documents else 0}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ\n",
    "    file_paths = [\"../book2.pdf\"]\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã)\n",
    "    print(\"üéØ –í–∞—Ä–∏–∞–Ω—Ç 1: –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º\")\n",
    "    docs_by_pages = load_and_prepare_pdfs(\n",
    "        file_paths,\n",
    "        max_pages=10,  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ—Å—Ç–∞\n",
    "        chunk_strategy='pages'\n",
    "    )\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º (–Ω–µ –æ–±—Ä–µ–∑–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)\n",
    "    print(\"\\nüéØ –í–∞—Ä–∏–∞–Ω—Ç 2: –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º\")\n",
    "    docs_by_sentences = load_and_prepare_pdfs(\n",
    "        file_paths,\n",
    "        max_pages=10,\n",
    "        chunk_strategy='sentences'\n",
    "    )\n",
    "    \n",
    "    # –í–∞—Ä–∏–∞–Ω—Ç 3: –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä)\n",
    "    print(\"\\nüéØ –í–∞—Ä–∏–∞–Ω—Ç 3: –£–º–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ\")\n",
    "    docs_smart = load_and_prepare_pdfs(\n",
    "        file_paths,\n",
    "        max_pages=10,\n",
    "        chunk_strategy='smart'\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
